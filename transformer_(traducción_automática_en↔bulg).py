# -*- coding: utf-8 -*-
"""Transformer (Traducción Automática EN↔ES).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AlGgj8wPFdZYFhlXAfVMTzA4fcyjEmkF
"""

pip install -q sentencepiece torchtext sacrebleu tqdm

########################################################################
# Sección 1 - Importar librerías y configuración general
########################################################################


import os
import time
import math
import random
from pathlib import Path


import pandas as pd
import sentencepiece as spm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence


from tqdm.auto import tqdm


# BLEU
import sacrebleu


# Configuración: detectar GPU
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device:', DEVICE)

########################################################################
# Sección 2 - Cargar y explorar el dataset
########################################################################


# Supone que english_bulgarian.csv está en el directorio de trabajo
DATA_CSV = 'english_bulgarian.csv'
assert os.path.exists(DATA_CSV), f"No se encontró {DATA_CSV} en el directorio actual"


# El CSV esperado tiene dos columnas: english, bulgarian (si no, adaptar)
df = pd.read_csv(DATA_CSV)
print('Columns:', df.columns.tolist())
print('Número de filas:', len(df))


# Mostrar primeras filas
print(df.head(5))

########################################################################
# Sección 3 - Preprocesamiento: limpieza y normalización
########################################################################

import re

def normalize_text(s):
    s = str(s).lower()
    # eliminar múltiples espacios
    s = re.sub(r"\s+", " ", s).strip()
    # opcional: eliminar caracteres no imprimibles
    s = re.sub(r"[^\x00-\x7f]+", " ", s)  # con búlgaro podría eliminar cirílico
    return s

# IMPORTANTE: El dataset es English-Bulgarian. No eliminar caracteres cirílicos en búlgaro.
# Normalización suave: lower para inglés, strip para búlgaro.

def normalize_row(en, bg):
    en_n = str(en).lower()
    en_n = re.sub(r"\s+", " ", en_n).strip()

    # Para búlgaro, solo limpiar espacios extra sin eliminar no-ascii
    bg_n = str(bg).strip()
    bg_n = re.sub(r"\s+", " ", bg_n)

    return en_n, bg_n

# Aplicar y filtrar filas vacías
pairs = []
for _, row in df.iterrows():

    # intentar encontrar columnas por nombre
    if 'english' in df.columns.str.lower().tolist():
        pass

    # Soporte flexible: tomar las dos primeras columnas
    if len(df.columns) >= 2:
        en = row.iloc[0]
        bg = row.iloc[1]
    else:
        continue

    en_n, bg_n = normalize_row(en, bg)

    if len(en_n.split()) < 1 or len(bg_n.split()) < 1:
        continue

    pairs.append((en_n, bg_n))

print('Pares válidos:', len(pairs))

# Opcional: tomar una porción si el dataset es demasiado grande (para pruebas)
MAX_PAIRS = 20000  # establecer, p.ej.: 50000
if MAX_PAIRS:
    pairs = pairs[:MAX_PAIRS]

########################################################################
# Sección 4 - Split: train / val / test
########################################################################


random.shuffle(pairs)
N = len(pairs)
train_end = int(0.9 * N)
val_end = int(0.95 * N)
train_pairs = pairs[:train_end]
val_pairs = pairs[train_end:val_end]
test_pairs = pairs[val_end:]


print('Train:', len(train_pairs), 'Val:', len(val_pairs), 'Test:', len(test_pairs))

########################################################################
# Sección 5 - Tokenización con SentencePiece
########################################################################

# Creamos un archivo temporal con textos para entrenar el modelo de SentencePiece
# (concatenado de ambos idiomas)
spm_input = 'spm_corpus.txt'
with open(spm_input, 'w', encoding='utf-8') as f:
    for en, bg in train_pairs:
        f.write(en + '\n')
        f.write(bg + '\n')

VOCAB_SIZE = 16000  # tamaño del vocabulario; ajustar si es necesario
spm_model_prefix = 'spm_model'

# Entrenar SentencePiece (utf-8)
spm.SentencePieceTrainer.Train(
    input=spm_input,
    model_prefix=spm_model_prefix,
    vocab_size=VOCAB_SIZE,
    character_coverage=0.9995,  # alto para cubrir cirílico
    model_type='unigram'        # o 'bpe'
)

sp = spm.SentencePieceProcessor()
sp.Load(spm_model_prefix + '.model')
print('SentencePiece cargado. Vocab size real:', sp.GetPieceSize())

# Helper: obtener BOS/EOS desde el modelo si existen
BOS_ID = sp.PieceToId('<s>') if '<s>' in [sp.IdToPiece(i) for i in range(sp.GetPieceSize())] else None
EOS_ID = sp.PieceToId('</s>') if '</s>' in [sp.IdToPiece(i) for i in range(sp.GetPieceSize())] else None

# Si no existen, asignamos IDs especiales reservados
if BOS_ID is None or EOS_ID is None:
    BOS_ID = sp.GetPieceSize()      # appended
    EOS_ID = sp.GetPieceSize() + 1  # appended
    # Nota: no se añaden al modelo; luego se ajustarán embeddings

print('BOS_ID, EOS_ID:', BOS_ID, EOS_ID)

def encode_text(s):
    # Devuelve torch.tensor de ids sin añadir BOS/EOS
    ids = sp.EncodeAsIds(s)
    return ids

# Probar tokenización
print('Ejemplo tokenización EN:', train_pairs[0][0])
print(sp.EncodeAsPieces(train_pairs[0][0])[:20])

########################################################################
# Sección 6 - Dataset y DataLoader
########################################################################

class TranslationDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        en, bg = self.pairs[idx]
        src_ids = encode_text(en)
        tgt_ids = encode_text(bg)
        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)


def collate_fn(batch):
    src_batch = [item[0] for item in batch]
    tgt_batch = [item[1] for item in batch]

    # añadir EOS para objetivo en entrenamiento y BOS para el decodificador si se requiere
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)

    return src_padded, tgt_padded


BATCH_SIZE = 64

train_dataset = TranslationDataset(train_pairs)
val_dataset = TranslationDataset(val_pairs)
test_dataset = TranslationDataset(test_pairs)

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn
)

########################################################################
# Sección 7 - Implementación del Transformer (simplificado)
########################################################################

# Usaremos nn.Embedding con vocab_size aumentado si usamos BOS/EOS externamente
VOCAB_SIZE_REAL = sp.GetPieceSize()
EXTRA_TOKENS = 2  # si BOS/EOS están fuera del vocab
TOTAL_VOCAB = VOCAB_SIZE_REAL + EXTRA_TOKENS

EMB_SIZE = 256
NHEAD = 8
FFN_HID_DIM = 512
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3
DROPOUT = 0.1
MAX_SEQ_LEN = 256


class PositionalEncoding(nn.Module):
    def __init__(self, emb_size: int, maxlen: int = 5000):
        super().__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)

        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)

        pos_embedding = pos_embedding.unsqueeze(0)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: torch.Tensor):
        return token_embedding + self.pos_embedding[:, :token_embedding.size(1), :]


class SimpleTransformer(nn.Module):
    def __init__(
        self,
        vocab_size,
        emb_size=EMB_SIZE,
        nhead=NHEAD,
        num_encoder_layers=NUM_ENCODER_LAYERS,
        num_decoder_layers=NUM_DECODER_LAYERS,
        dim_feedforward=FFN_HID_DIM,
        dropout=DROPOUT
    ):
        super().__init__()

        self.src_tok_emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)
        self.tgt_tok_emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)

        self.positional_encoding = PositionalEncoding(emb_size, maxlen=MAX_SEQ_LEN)

        self.transformer = nn.Transformer(
            d_model=emb_size,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )

        self.generator = nn.Linear(emb_size, vocab_size)
        self.emb_size = emb_size

    def generate_square_subsequent_mask(self, size):
        mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)
        return mask

    def forward(self, src, tgt):
        # src: (batch, src_len), tgt: (batch, tgt_len)
        src_mask = None
        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)

        src_key_padding_mask = (src == 0)
        tgt_key_padding_mask = (tgt == 0)

        src_emb = self.src_tok_emb(src) * math.sqrt(self.emb_size)
        tgt_emb = self.tgt_tok_emb(tgt) * math.sqrt(self.emb_size)

        src_emb = self.positional_encoding(src_emb)
        tgt_emb = self.positional_encoding(tgt_emb)

        # PyTorch Transformer espera (seq_len, batch, emb)
        src_emb = src_emb.transpose(0, 1)
        tgt_emb = tgt_emb.transpose(0, 1)

        output = self.transformer(
            src_emb,
            tgt_emb,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )

        output = output.transpose(0, 1)
        return self.generator(output)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


model = SimpleTransformer(TOTAL_VOCAB)
print('Parámetros del modelo:', count_parameters(model))

########################################################################
# Sección 8 - Funciones auxiliares: preparar batch de entrada/salida para el Transformer
########################################################################

def prepare_batch(src_batch, tgt_batch, device=DEVICE):
    # src_batch, tgt_batch: (batch, seq_len), padded, torch.long
    # Para entrenamiento, el input del decoder es tgt[:-1] y la target es tgt[1:]

    src = src_batch.to(device)
    tgt = tgt_batch.to(device)

    # Aquí no añadimos BOS/EOS; simplemente desplazamos tal cual.
    tgt_input = tgt
    tgt_output = tgt

    return src, tgt_input, tgt_output

########################################################################
# Sección 9 - Entrenamiento (loop sencillo)
########################################################################

EPOCHS = 10
LEARNING_RATE = 1e-4
CLIP_GRAD_NORM = 1.0

criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

train_losses = []
val_losses = []

for epoch in range(1, EPOCHS + 1):
    model.train()
    epoch_loss = 0.0
    start_time = time.time()

    pbar = tqdm(train_loader, desc=f'Epoch {epoch} training')
    for src_batch, tgt_batch in pbar:
        src, tgt_input, tgt_output = prepare_batch(src_batch, tgt_batch)

        optimizer.zero_grad()

        # Forward: logits -> (batch, tgt_len, vocab)
        logits = model(src, tgt_input)

        # Flatten para pérdida
        logits_flat = logits.reshape(-1, logits.size(-1))
        tgt_flat = tgt_output.reshape(-1).to(DEVICE)

        loss = criterion(logits_flat, tgt_flat)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)
        optimizer.step()

        epoch_loss += loss.item()
        pbar.set_postfix({'loss': loss.item()})

    epoch_loss = epoch_loss / len(train_loader)
    train_losses.append(epoch_loss)

    # ===============================
    # Validación
    # ===============================
    model.eval()
    val_loss = 0.0

    with torch.no_grad():
        for src_batch, tgt_batch in val_loader:
            src, tgt_input, tgt_output = prepare_batch(src_batch, tgt_batch)

            logits = model(src, tgt_input)
            logits_flat = logits.reshape(-1, logits.size(-1))
            tgt_flat = tgt_output.reshape(-1).to(DEVICE)

            loss = criterion(logits_flat, tgt_flat)
            val_loss += loss.item()

    val_loss = val_loss / len(val_loader)
    val_losses.append(val_loss)

    elapsed = time.time() - start_time

    print(
        f"Epoch {epoch} -> "
        f"train_loss: {epoch_loss:.4f}, "
        f"val_loss: {val_loss:.4f}, "
        f"tiempo: {elapsed:.1f}s"
    )

########################################################################
# Sección 10 - Decodificación / Inferencia (greedy) y evaluación BLEU
########################################################################

def greedy_decode(model, src_sentence, max_len=100):
    # src_sentence: lista de token ids (sin padding)
    model.eval()

    src = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(DEVICE)  # (1, src_len)

    generated = []  # tokens generados

    for _ in range(max_len):
        if len(generated) > 0:
            tgt_input = torch.tensor([generated], dtype=torch.long).to(DEVICE)
        else:
            tgt_input = torch.zeros((1, 1), dtype=torch.long).to(DEVICE)

        logits = model(src, tgt_input)  # (1, tgt_len, vocab)
        next_token_logits = logits[0, -1, :]
        next_id = torch.argmax(next_token_logits).item()

        # Si el modelo genera 0, interpretamos como padding/EOS
        if next_id == 0:
            break

        generated.append(next_id)

    return generated


# Preparar listas de referencias y predicciones para BLEU
refs = []
hyps = []

# Tomar una parte del test para acelerar la evaluación
EVAL_SAMPLES = min(500, len(test_pairs))

for i in tqdm(range(EVAL_SAMPLES), desc='Evaluación BLEU'):
    en, bg = test_pairs[i]

    src_ids = encode_text(en)
    pred_ids = greedy_decode(model, src_ids, max_len=80)

    # Convertir ids → texto
    pred_text = sp.DecodeIds(pred_ids) if len(pred_ids) > 0 else ''

    refs.append(bg)
    hyps.append(pred_text)


# Calcular BLEU con sacrebleu
bleu = sacrebleu.corpus_bleu(hyps, [refs])
print('BLEU score:', bleu.score)

########################################################################
# Sección 11 - Métricas de recursos y resumen
########################################################################

num_params = count_parameters(model)
print(f"Parámetros entrenables: {num_params}")
print('Train losses por epoch:', train_losses)
print('Val losses por epoch:', val_losses)
print('BLEU en test (muestra):', bleu.score)

# Guardar modelo
MODEL_PATH = 'transformer_model.pth'
torch.save(model.state_dict(), MODEL_PATH)
print('Modelo guardado en', MODEL_PATH)
