# -*- coding: utf-8 -*-
"""GRU-English_Bulgar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C5VM35IgWWjrqnxydNkThhrNibSACLLv
"""

# ====================== PARTE 1: CARGA Y PREPARACIÓN RÁPIDA (SOLUCIONADO) ======================
import pandas as pd
import torch
import sentencepiece as spm
from sacremoses import MosesPunctNormalizer
import numpy as np
import random
from tqdm import tqdm
import os

# Reproducibilidad
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Usando: {device}")

# 1. CARGAR EL CSV (ya está en Colab)
df = pd.read_csv('english_bulgarian.csv').dropna().reset_index(drop=True)
print(f"Total de pares: {len(df):,}")

# Asegurar nombres de columnas
if df.shape[1] == 2:
    df.columns = ['english', 'bulgarian']

# 2. NORMALIZACIÓN RÁPIDA
normalizer = MosesPunctNormalizer()
print("Normalizando texto (esto toma ~10-20 segundos)...")
df['en'] = df['english'].astype(str).apply(lambda x: normalizer.normalize(x).lower())
df['bg'] = df['bulgarian'].astype(str).apply(lambda x: normalizer.normalize(x).lower())

# Estadísticas
df['len_en'] = df['en'].str.split().str.len()
df['len_bg'] = df['bg'].str.split().str.len()
print(f"Longitud media → EN: {df['len_en'].mean():.1f} | BG: {df['len_bg'].mean():.1f}")

# 3. ENTRENAR SENTENCEPIECE EN SEGUNDOS (truco profesional)
print("Entrenando SentencePiece BPE de 16k (rápido y sin memoria loca)...")

# Guardamos solo lo necesario en archivos temporales (es la forma MÁS RÁPIDA y usada en producción)
with open('/tmp/train_en.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(df['en'].tolist()))
with open('/tmp/train_bg.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(df['bg'].tolist()))

# Entrenamos con los dos archivos (así es 20x más rápido)
spm.SentencePieceTrainer.train(
    '--input=/tmp/train_en.txt,/tmp/train_bg.txt '
    '--model_prefix=bpe_model '
    '--vocab_size=16000 '
    '--model_type=bpe '
    '--character_coverage=1.0 '
    '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3'
)

# Cargar modelo
sp = spm.SentencePieceProcessor(model_file='bpe_model.model')
VOCAB_SIZE = sp.get_piece_size()
SOS, EOS, PAD = sp.bos_id(), sp.eos_id(), sp.pad_id()

print(f"Vocabulario BPE creado: {VOCAB_SIZE} subpalabras → Listo en segundos!")

# Opcional: borrar archivos temporales
os.remove('/tmp/train_en.txt')
os.remove('/tmp/train_bg.txt')

# ====================== PARTE 2: DATASET Y DATALOADERS ======================
MAX_LEN = 60

class TranslationDataset(Dataset):
    def __init__(self, en_texts, bg_texts):
        self.src = []
        self.trg = []
        for en, bg in zip(en_texts, bg_texts):
            src_ids = sp.encode_as_ids(en)
            trg_ids = sp.encode_as_ids(bg)
            if len(src_ids) <= MAX_LEN and len(trg_ids) <= MAX_LEN and len(src_ids) > 0:
                self.src.append(torch.tensor([SOS] + src_ids + [EOS]))
                self.trg.append(torch.tensor([SOS] + trg_ids + [EOS]))
        print(f"Oraciones válidas (≤{MAX_LEN} tokens): {len(self.src):,}")

    def __len__(self): return len(self.src)
    def __getitem__(self, i): return self.src[i], self.trg[i]

def collate_batch(batch):
    src, trg = zip(*batch)
    return (torch.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=PAD),
            torch.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=PAD))

# Crear dataset completo y dividir
full_dataset = TranslationDataset(df['en'].tolist(), df['bg'].tolist())
train_size = int(0.90 * len(full_dataset))
val_size   = int(0.05 * len(full_dataset))
test_size  = len(full_dataset) - train_size - val_size

train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])
print(f"Train: {len(train_data):,} | Val: {len(val_data):,} | Test: {len(test_data):,}")

BATCH_SIZE = 64
train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_batch)
val_loader   = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)
test_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)

MAX_SAMPLES = 20000
train_data.indices = train_data.indices[:MAX_SAMPLES]
val_data.indices   = val_data.indices[:3000]
test_data.indices  = test_data.indices[:3000]
print(f"DATASET REDUCIDO → Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}")
print("Entrenamiento ultrarrápido activado")

# ====================== MODELO + ENTRENAMIENTO ======================
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import math
import time
import matplotlib.pyplot as plt
from tqdm import tqdm

class EncoderGRU(nn.Module):
    def __init__(self, vocab_size, emb_dim=256, hid_dim=512, n_layers=2, dropout=0.5):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)
        self.gru = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, bidirectional=True,
                          dropout=dropout if n_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hid_dim * 2, hid_dim)  # 1024 → 512
        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        embedded = self.dropout(self.emb(src))
        outputs, hidden = self.gru(embedded)  # outputs [B, L, 1024], hidden [4, B, 512]
        # Combinar forward + backward
        h = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [B, 1024]
        h = self.fc(h)  # [B, 512]
        hidden = h.unsqueeze(0).repeat(2, 1, 1)  # [2, B, 512] → para 2 capas del decoder
        return outputs, hidden

class Attention(nn.Module):
    def __init__(self, hid_dim):
        super().__init__()
        self.attn = nn.Linear(hid_dim + hid_dim*2, hid_dim)
        self.v = nn.Parameter(torch.rand(hid_dim))

    def forward(self, hidden, encoder_outputs):
        # hidden: [2, B, 512] → tomamos última capa
        hidden = hidden[-1].unsqueeze(1)  # [B, 1, 512]
        seq_len = encoder_outputs.size(1)
        hidden = hidden.repeat(1, seq_len, 1)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        attention = torch.softmax(torch.matmul(energy, self.v), dim=1)
        return attention  # [B, seq_len]

class DecoderGRU(nn.Module):
    def __init__(self, vocab_size, emb_dim=256, hid_dim=512, n_layers=2, dropout=0.5):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)
        self.attention = Attention(hid_dim)
        self.gru = nn.GRU(emb_dim + hid_dim*2, hid_dim, num_layers=n_layers,
                          dropout=dropout if n_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hid_dim, vocab_size)  # ← SOLUCIÓN: solo el output del GRU
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, encoder_outputs):
        input = input.unsqueeze(1)  # [B, 1]
        embedded = self.dropout(self.emb(input))  # [B, 1, 256]

        attn_weights = self.attention(hidden, encoder_outputs)  # [B, src_len]
        attn_weights = attn_weights.unsqueeze(1)
        context = torch.bmm(attn_weights, encoder_outputs)  # [B, 1, 1024]

        rnn_input = torch.cat((embedded, context), dim=2)
        output, hidden = self.gru(rnn_input, hidden)
        prediction = self.fc(output.squeeze(1))  # ← ahora sí: 512 → vocab_size
        return prediction, hidden

class Seq2SeqGRU(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, trg, teacher_forcing=0.5):
        batch_size = src.size(0)
        trg_len = trg.size(1)
        outputs = torch.zeros(batch_size, trg_len, VOCAB_SIZE).to(device)

        encoder_outputs, hidden = self.encoder(src)
        input = trg[:, 0]

        for t in range(1, trg_len):
            output, hidden = self.decoder(input, hidden, encoder_outputs)
            outputs[:, t] = output
            teacher = random.random() < teacher_forcing
            input = trg[:, t] if teacher else output.argmax(1)
        return outputs

# ====================== INSTANCIAR MODELO ======================
encoder = EncoderGRU(VOCAB_SIZE)
decoder = DecoderGRU(VOCAB_SIZE)
model = Seq2SeqGRU(encoder, decoder).to(device)

print(f"Parámetros entrenables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss(ignore_index=PAD)

# ====================== ENTRENAMIENTO 10 ÉPOCAS ======================
train_losses, val_losses = [], []

print("\n" + "="*80)
print("ENTRENAMIENTO FINAL - GRU + ATENCIÓN (INGLÉS → BÚLGARO) - 100% FUNCIONAL")
print("="*80)

start_time = time.time()
for epoch in range(1, 11):
    model.train()
    epoch_loss = 0
    for src, trg in tqdm(train_loader, desc=f"Época {epoch}/10", leave=False):
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()
        output = model(src, trg, 0.5)
        loss = criterion(output[:,1:].reshape(-1, VOCAB_SIZE), trg[:,1:].reshape(-1))
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
        optimizer.step()
        epoch_loss += loss.item()
    train_losses.append(epoch_loss / len(train_loader))

    model.eval()
    val_loss = 0
    with torch.no_grad():
        for src, trg in val_loader:
            src, trg = src.to(device), trg.to(device)
            output = model(src, trg, 0)
            val_loss += criterion(output[:,1:].reshape(-1, VOCAB_SIZE), trg[:,1:].reshape(-1)).item()
    val_loss /= len(val_loader)
    val_losses.append(val_loss)
    print(f"Época {epoch:02d} → Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_loss:.4f} | Val PPL: {math.exp(val_loss):.1f}")

print(f"\nENTRENAMIENTO TERMINADO EN {time.time()-start_time:.1f} segundos")

# Gráfico
plt.figure(figsize=(10,5))
plt.plot(train_losses, 'o-', label='Train Loss')
plt.plot(val_losses, 's-', label='Validation Loss')
plt.title('GRU + Atención Bahdanau - Inglés → Búlgaro (10 épocas)')
plt.xlabel('Época'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)
plt.savefig('GRU_Attention_Loss.png', dpi=200)
plt.show()

# Gráfico
plt.figure(figsize=(10,5))
plt.plot(range(1,11), train_losses, 'o-', label='Train Loss', linewidth=2)
plt.plot(range(1,11), val_losses, 's-', label='Validation Loss', linewidth=2)
plt.title('GRU bidireccional + Atención Bahdanau - 10 épocas', fontsize=14)
plt.xlabel('Época'); plt.ylabel('Loss'); plt.legend(); plt.grid(True, alpha=0.3)
plt.savefig('gru_attention_loss.png', dpi=200, bbox_inches='tight')
plt.show()